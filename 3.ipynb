{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, input, init_state):\n",
    "        embedding = self.embedding(input)\n",
    "        output, state_h, state_c = self.lstm(embedding, initial_state=init_state)\n",
    "        return output, state_h, state_c\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        return (tf.zeros([batch_size, self.hidden_units]), tf.zeros([batch_size, self.hidden_units]))\n",
    "    \n",
    "    def predict(self, str):\n",
    "        seq = []\n",
    "        global word2idx_input\n",
    "        for c in list(str):\n",
    "            seq.append(word2idx_input[c])\n",
    "        input = [seq]\n",
    "        init_state = self.init_state(1)\n",
    "        output = self.call(tf.constant(input), init_state)\n",
    "        return output[1:]\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "        self.dense = keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, input, init_state):\n",
    "        embedding = self.embedding(input)\n",
    "        output, state_h, state_c = self.lstm(embedding, init_state)\n",
    "        result =  self.dense(output)\n",
    "        return result, state_h, state_c\n",
    "    \n",
    "    def predict(self, init_state):\n",
    "        global word2idx_output, idx2word_output\n",
    "        input = tf.constant([[word2idx_output['<']]])\n",
    "        seq = []\n",
    "        global max_len_input\n",
    "        for _ in range(max_len_input):\n",
    "            output = self.call(input, init_state)\n",
    "            input = tf.argmax(output[0], axis=-1)\n",
    "            seq.append(idx2word_output[input.numpy()[0][0]])\n",
    "        return ''.join(seq)\n",
    "\n",
    "def get_loss(y_true, y_pred):\n",
    "    crossentropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    loss = crossentropy(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    labels = tf.cast(tf.argmax(y_pred, axis=-1), dtype='int32')\n",
    "    correct = tf.cast(tf.equal(y_true, labels), dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "X = train_data['Sentence'].to_numpy()\n",
    "Y = train_data['Transformed sentence'].apply(lambda x: x+'>').to_numpy()\n",
    "Z = train_data['Transformed sentence'].apply(lambda x: '<'+x).to_numpy()\n",
    "\n",
    "# Data preprocessing - convert strings to ids and back\n",
    "x_tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "y_tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "x_tokenizer.fit_on_texts(X)\n",
    "y_tokenizer.fit_on_texts(Y)\n",
    "y_tokenizer.fit_on_texts(Z)\n",
    "vocab_size = len(y_tokenizer.word_index) + 1\n",
    "\n",
    "word2idx_input = x_tokenizer.word_index\n",
    "word2idx_output = y_tokenizer.word_index\n",
    "idx2word_input = {x:y for y,x in word2idx_input.items()}\n",
    "idx2word_output = {x:y for y,x in word2idx_output.items()}\n",
    "\n",
    "num_input = len(word2idx_input) + 1\n",
    "num_output = len(word2idx_output) + 1\n",
    "\n",
    "x_train = x_tokenizer.texts_to_sequences(X)\n",
    "y_train = y_tokenizer.texts_to_sequences(Y)\n",
    "z_train = y_tokenizer.texts_to_sequences(Z)\n",
    "max_len_input = max(len(i) for i in x_train)\n",
    "max_len_output = max(len(i) for i in y_train)\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_len_input, padding='post')\n",
    "y_train = keras.preprocessing.sequence.pad_sequences(y_train, maxlen=max_len_output, padding='post')\n",
    "z_train = keras.preprocessing.sequence.pad_sequences(z_train, maxlen=max_len_output, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Loss: 2.927985 Accuracy: 0.151679 Time: 52.1373\n",
      "Epoch: 2/100 Loss: 2.741571 Accuracy: 0.185955 Time: 51.0065\n",
      "Epoch: 3/100 Loss: 2.534233 Accuracy: 0.231237 Time: 49.7697\n",
      "Epoch: 4/100 Loss: 2.436774 Accuracy: 0.258235 Time: 50.2250\n",
      "Epoch: 5/100 Loss: 2.376010 Accuracy: 0.271359 Time: 50.0643\n",
      "Epoch: 6/100 Loss: 2.333196 Accuracy: 0.280836 Time: 50.1846\n",
      "Epoch: 7/100 Loss: 2.298186 Accuracy: 0.289644 Time: 81.9057\n",
      "Epoch: 8/100 Loss: 2.260527 Accuracy: 0.300395 Time: 63.5493\n",
      "Epoch: 9/100 Loss: 2.219035 Accuracy: 0.314236 Time: 62.4739\n",
      "Epoch: 10/100 Loss: 2.167199 Accuracy: 0.327902 Time: 63.2900\n",
      "Epoch: 11/100 Loss: 2.107433 Accuracy: 0.340501 Time: 62.3372\n",
      "Epoch: 12/100 Loss: 2.038729 Accuracy: 0.356189 Time: 65.2866\n",
      "Epoch: 13/100 Loss: 1.965909 Accuracy: 0.370715 Time: 59.9875\n",
      "Epoch: 14/100 Loss: 1.887421 Accuracy: 0.387551 Time: 51.7992\n",
      "Epoch: 15/100 Loss: 1.805542 Accuracy: 0.401870 Time: 53.1487\n",
      "Epoch: 16/100 Loss: 1.718843 Accuracy: 0.421955 Time: 51.8538\n",
      "Epoch: 17/100 Loss: 1.643712 Accuracy: 0.436258 Time: 50.2143\n",
      "Epoch: 18/100 Loss: 1.569999 Accuracy: 0.449541 Time: 50.9111\n",
      "Epoch: 19/100 Loss: 1.504422 Accuracy: 0.462411 Time: 51.8747\n",
      "Epoch: 20/100 Loss: 1.440766 Accuracy: 0.480250 Time: 53.1553\n",
      "Epoch: 21/100 Loss: 1.384824 Accuracy: 0.492227 Time: 51.3072\n",
      "Epoch: 22/100 Loss: 1.327995 Accuracy: 0.504078 Time: 52.2121\n",
      "Epoch: 23/100 Loss: 1.280092 Accuracy: 0.514972 Time: 51.8885\n",
      "Epoch: 24/100 Loss: 1.233177 Accuracy: 0.527045 Time: 52.0798\n",
      "Epoch: 25/100 Loss: 1.189951 Accuracy: 0.539262 Time: 52.0592\n",
      "Epoch: 26/100 Loss: 1.148888 Accuracy: 0.548786 Time: 51.1222\n",
      "Epoch: 27/100 Loss: 1.107929 Accuracy: 0.561114 Time: 59.0605\n",
      "Epoch: 28/100 Loss: 1.075921 Accuracy: 0.570958 Time: 64.6408\n",
      "Epoch: 29/100 Loss: 1.037495 Accuracy: 0.581788 Time: 61.5255\n",
      "Epoch: 30/100 Loss: 1.006545 Accuracy: 0.592667 Time: 65.7695\n",
      "Epoch: 31/100 Loss: 0.982139 Accuracy: 0.598146 Time: 68.1292\n",
      "Epoch: 32/100 Loss: 0.954885 Accuracy: 0.608372 Time: 65.8133\n",
      "Epoch: 33/100 Loss: 0.926381 Accuracy: 0.617291 Time: 56.1808\n",
      "Epoch: 34/100 Loss: 0.904465 Accuracy: 0.627915 Time: 71.0599\n",
      "Epoch: 35/100 Loss: 0.884691 Accuracy: 0.635289 Time: 90.5430\n",
      "Epoch: 36/100 Loss: 0.857384 Accuracy: 0.644464 Time: 94.0921\n",
      "Epoch: 37/100 Loss: 0.837923 Accuracy: 0.652141 Time: 91.9673\n",
      "Epoch: 38/100 Loss: 0.814390 Accuracy: 0.660200 Time: 91.1779\n",
      "Epoch: 39/100 Loss: 0.800427 Accuracy: 0.668260 Time: 92.1227\n",
      "Epoch: 40/100 Loss: 0.778273 Accuracy: 0.675873 Time: 94.6966\n",
      "Epoch: 41/100 Loss: 0.763428 Accuracy: 0.683343 Time: 87.8104\n",
      "Epoch: 42/100 Loss: 0.743726 Accuracy: 0.691450 Time: 93.7544\n",
      "Epoch: 43/100 Loss: 0.725751 Accuracy: 0.696833 Time: 92.2525\n",
      "Epoch: 44/100 Loss: 0.718013 Accuracy: 0.702010 Time: 49.5314\n",
      "Epoch: 45/100 Loss: 0.694566 Accuracy: 0.713590 Time: 40.2118\n",
      "Epoch: 46/100 Loss: 0.686303 Accuracy: 0.717221 Time: 45.2369\n",
      "Epoch: 47/100 Loss: 0.671347 Accuracy: 0.724484 Time: 47.1776\n",
      "Epoch: 48/100 Loss: 0.657184 Accuracy: 0.731444 Time: 49.6435\n",
      "Epoch: 49/100 Loss: 0.640953 Accuracy: 0.738819 Time: 48.0885\n",
      "Epoch: 50/100 Loss: 0.628533 Accuracy: 0.745381 Time: 45.9084\n",
      "Epoch: 51/100 Loss: 0.617525 Accuracy: 0.751497 Time: 48.6264\n",
      "Epoch: 52/100 Loss: 0.610520 Accuracy: 0.754635 Time: 45.8282\n",
      "Epoch: 53/100 Loss: 0.587604 Accuracy: 0.763618 Time: 44.8032\n",
      "Epoch: 54/100 Loss: 0.581348 Accuracy: 0.770483 Time: 48.4566\n",
      "Epoch: 55/100 Loss: 0.564585 Accuracy: 0.776519 Time: 56.6289\n",
      "Epoch: 56/100 Loss: 0.555274 Accuracy: 0.778861 Time: 58.3359\n",
      "Epoch: 57/100 Loss: 0.538923 Accuracy: 0.787733 Time: 55.4175\n",
      "Epoch: 58/100 Loss: 0.528657 Accuracy: 0.794120 Time: 46.9801\n",
      "Epoch: 59/100 Loss: 0.530585 Accuracy: 0.793578 Time: 48.2989\n",
      "Epoch: 60/100 Loss: 0.513137 Accuracy: 0.801669 Time: 47.6873\n",
      "Epoch: 61/100 Loss: 0.499473 Accuracy: 0.807769 Time: 45.2579\n",
      "Epoch: 62/100 Loss: 0.488089 Accuracy: 0.813838 Time: 44.3507\n",
      "Epoch: 63/100 Loss: 0.478610 Accuracy: 0.817470 Time: 44.2464\n",
      "Epoch: 64/100 Loss: 0.464938 Accuracy: 0.824924 Time: 38.7580\n",
      "Epoch: 65/100 Loss: 0.459620 Accuracy: 0.826644 Time: 38.7136\n",
      "Epoch: 66/100 Loss: 0.452432 Accuracy: 0.832282 Time: 38.3573\n",
      "Epoch: 67/100 Loss: 0.439913 Accuracy: 0.835977 Time: 38.7535\n",
      "Epoch: 68/100 Loss: 0.432103 Accuracy: 0.843049 Time: 38.8174\n",
      "Epoch: 69/100 Loss: 0.420518 Accuracy: 0.844100 Time: 38.8900\n",
      "Epoch: 70/100 Loss: 0.407464 Accuracy: 0.852638 Time: 39.4629\n",
      "Epoch: 71/100 Loss: 0.401551 Accuracy: 0.855234 Time: 39.0819\n",
      "Epoch: 72/100 Loss: 0.404080 Accuracy: 0.853020 Time: 39.3140\n",
      "Epoch: 73/100 Loss: 0.385004 Accuracy: 0.862815 Time: 39.5664\n",
      "Epoch: 74/100 Loss: 0.364879 Accuracy: 0.873550 Time: 39.6564\n",
      "Epoch: 75/100 Loss: 0.366220 Accuracy: 0.871130 Time: 39.5461\n",
      "Epoch: 76/100 Loss: 0.357171 Accuracy: 0.876035 Time: 39.4969\n",
      "Epoch: 77/100 Loss: 0.354758 Accuracy: 0.877628 Time: 39.9256\n",
      "Epoch: 78/100 Loss: 0.345062 Accuracy: 0.881275 Time: 40.0204\n",
      "Epoch: 79/100 Loss: 0.337153 Accuracy: 0.884206 Time: 40.2950\n",
      "Epoch: 80/100 Loss: 0.330102 Accuracy: 0.888586 Time: 40.5633\n",
      "Epoch: 81/100 Loss: 0.312363 Accuracy: 0.897506 Time: 40.4272\n",
      "Epoch: 82/100 Loss: 0.316017 Accuracy: 0.894607 Time: 40.9345\n",
      "Epoch: 83/100 Loss: 0.303778 Accuracy: 0.900755 Time: 40.6573\n",
      "Epoch: 84/100 Loss: 0.302748 Accuracy: 0.901121 Time: 40.9383\n",
      "Epoch: 85/100 Loss: 0.290050 Accuracy: 0.906298 Time: 40.9850\n",
      "Epoch: 86/100 Loss: 0.287082 Accuracy: 0.908432 Time: 41.0654\n",
      "Epoch: 87/100 Loss: 0.280259 Accuracy: 0.909961 Time: 41.3252\n",
      "Epoch: 88/100 Loss: 0.270786 Accuracy: 0.914277 Time: 41.6796\n",
      "Epoch: 89/100 Loss: 0.265849 Accuracy: 0.916284 Time: 41.6463\n",
      "Epoch: 90/100 Loss: 0.272511 Accuracy: 0.912876 Time: 41.4258\n",
      "Epoch: 91/100 Loss: 0.253678 Accuracy: 0.921381 Time: 41.8473\n",
      "Epoch: 92/100 Loss: 0.236926 Accuracy: 0.929855 Time: 41.5665\n",
      "Epoch: 93/100 Loss: 0.227088 Accuracy: 0.933677 Time: 41.5091\n",
      "Epoch: 94/100 Loss: 0.249329 Accuracy: 0.922496 Time: 43.4490\n",
      "Epoch: 95/100 Loss: 0.249016 Accuracy: 0.922289 Time: 52.1347\n",
      "Epoch: 96/100 Loss: 0.250796 Accuracy: 0.921668 Time: 46.8612\n",
      "Epoch: 97/100 Loss: 0.240512 Accuracy: 0.926908 Time: 43.9225\n",
      "Epoch: 98/100 Loss: 0.215839 Accuracy: 0.937914 Time: 46.1315\n",
      "Epoch: 99/100 Loss: 0.196088 Accuracy: 0.946436 Time: 43.3301\n",
      "Epoch: 100/100 Loss: 0.187350 Accuracy: 0.950386 Time: 42.0360\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "embed_size = 4096\n",
    "hidden_units = 128\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_units)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_units)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, z_train))\n",
    "dataset = dataset.shuffle(len(train_data))\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(clipnorm=5.0)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    total_loss, total_accuracy, num_batches = 0, 0, 0\n",
    "    start = time.time()\n",
    "    en_init = encoder.init_state(batch_size)\n",
    "    for batch, (x_train, y_train, z_train) in enumerate(dataset.take(-1)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            en_out = encoder(x_train, en_init)\n",
    "            de_init = en_out[1:]\n",
    "            de_out = decoder(z_train, de_init)\n",
    "            y_pred = de_out[0]\n",
    "            loss = get_loss(y_train, y_pred)\n",
    "            accuracy = get_accuracy(y_train, y_pred)\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "        num_batches += 1\n",
    "    end = time.time()\n",
    "    loss = total_loss / num_batches\n",
    "    accuracy = total_accuracy / num_batches\n",
    "    print('Epoch: {}/{} Loss: {:.6f} Accuracy: {:.6f} Time: {:.4f}'.format(i+1, n_epochs, loss, accuracy, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check how many characters match in the two strings\n",
    "def check(pred: str, true: str):\n",
    "    correct = 0\n",
    "    for a, b in zip(pred, true):\n",
    "        if a == b:\n",
    "            correct += 1\n",
    "\n",
    "    # Prediction is more than 8 letters, so penalize for every extra letter.\n",
    "    correct -= max(0, len(pred) - len(true))\n",
    "    correct = max(0, correct)\n",
    "    return correct\n",
    "\n",
    "# Function to score the model's performance\n",
    "def evaluate(encoder, decoder):\n",
    "\n",
    "    # Train data\n",
    "    print(\"Obtaining results for training data:\")\n",
    "    train_data = pd.read_csv(\"train_data.csv\").to_numpy()\n",
    "    results = {\n",
    "        \"pred\": [],\n",
    "        \"true\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    correct = [0 for _ in range(9)]\n",
    "    for x, y in train_data:\n",
    "        pred = decoder.predict(encoder.predict(x))\n",
    "        score = check(pred, y)\n",
    "        results[\"pred\"].append(pred)\n",
    "        results[\"true\"].append(y)\n",
    "        results[\"score\"].append(score)\n",
    "\n",
    "        correct[score] += 1\n",
    "    print(\"Train dataset results:\")\n",
    "    for num_chr in range(9):\n",
    "        print(\n",
    "            f\"Number of predictions with {num_chr} correct predictions: {correct[num_chr]}\"\n",
    "        )\n",
    "    points = sum(correct[4:6]) * 0.5 + sum(correct[6:])\n",
    "    print(f\"Points: {points}\")\n",
    "    # Save predicitons and true sentences to inspect manually if required.\n",
    "    pd.DataFrame.from_dict(results).to_csv(\"results_train.csv\", index=False)\n",
    "\n",
    "    #----------------------------------------------------------------------------------\n",
    "\n",
    "    print(\"Obtaining metrics for eval data:\")\n",
    "    eval_data = pd.read_csv(\"eval_data.csv\").to_numpy()\n",
    "    results = {\n",
    "        \"pred\": [],\n",
    "        \"true\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    correct = [0 for _ in range(9)]\n",
    "    for x, y in eval_data:\n",
    "        pred = decoder.predict(encoder.predict(x))\n",
    "        score = check(pred, y)\n",
    "        results[\"pred\"].append(pred)\n",
    "        results[\"true\"].append(y)\n",
    "        results[\"score\"].append(score)\n",
    "\n",
    "        correct[score] += 1\n",
    "    print(\"Eval dataset results:\")\n",
    "    for num_chr in range(9):\n",
    "        print(\n",
    "            f\"Number of predictions with {num_chr} correct predictions: {correct[num_chr]}\"\n",
    "        )\n",
    "    points = sum(correct[4:6]) * 0.5 + sum(correct[6:])\n",
    "    marks = round(min(2, points / 1400 * 2) * 2) / 2  # Rounds to the nearest 0.5\n",
    "    print(f\"Points: {points}\")\n",
    "    print(f\"Marks: {marks}\")\n",
    "    # Save predicitons and true sentences to inspect manually if required.\n",
    "    pd.DataFrame.from_dict(results).to_csv(\"results_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining results for training data:\n",
      "Train dataset results:\n",
      "Number of predictions with 0 correct predictions: 137\n",
      "Number of predictions with 1 correct predictions: 2946\n",
      "Number of predictions with 2 correct predictions: 2978\n",
      "Number of predictions with 3 correct predictions: 811\n",
      "Number of predictions with 4 correct predictions: 117\n",
      "Number of predictions with 5 correct predictions: 10\n",
      "Number of predictions with 6 correct predictions: 1\n",
      "Number of predictions with 7 correct predictions: 0\n",
      "Number of predictions with 8 correct predictions: 0\n",
      "Points: 64.5\n",
      "Obtaining metrics for eval data:\n",
      "Eval dataset results:\n",
      "Number of predictions with 0 correct predictions: 515\n",
      "Number of predictions with 1 correct predictions: 874\n",
      "Number of predictions with 2 correct predictions: 489\n",
      "Number of predictions with 3 correct predictions: 105\n",
      "Number of predictions with 4 correct predictions: 16\n",
      "Number of predictions with 5 correct predictions: 1\n",
      "Number of predictions with 6 correct predictions: 0\n",
      "Number of predictions with 7 correct predictions: 0\n",
      "Number of predictions with 8 correct predictions: 0\n",
      "Points: 8.5\n",
      "Marks: 0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_weights('encoder.h5')\n",
    "decoder.save_weights('decoder.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
